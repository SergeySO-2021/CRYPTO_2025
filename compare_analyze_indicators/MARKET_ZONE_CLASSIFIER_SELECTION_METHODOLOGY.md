# –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –≤—ã–±–æ—Ä–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —Ä—ã–Ω–æ—á–Ω—ã—Ö –∑–æ–Ω

## üéØ –¶–µ–ª—å –ø—Ä–æ–µ–∫—Ç–∞
**–≠—Ç–∞–ø 1:** –°—Ä–∞–≤–Ω–∏—Ç—å –∏ –≤—ã–±—Ä–∞—Ç—å –ª—É—á—à–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä—ã–Ω–æ—á–Ω—ã—Ö –∑–æ–Ω –∏–∑ —Ç—Ä–µ—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤.

**–û–±—â–∞—è —Ü–µ–ª—å –ø—Ä–æ–µ–∫—Ç–∞:** –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–æ–π —Å —É—á–µ—Ç–æ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä—ã–Ω–æ—á–Ω—ã—Ö –∑–æ–Ω.

## üìä –¢—Ä–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä—ã–Ω–æ—á–Ω—ã—Ö –∑–æ–Ω

### 1. Market Zone Analyzer (MZA) - –ß–µ—Ç—ã—Ä–µ—Ö—É—Ä–æ–≤–Ω–µ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞
**–û–ø–∏—Å–∞–Ω–∏–µ:** –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π Pine Script –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –¥–ª—è TradingView —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**
```
MZA = Trend Strength + Momentum + Price Action + Market Activity
```

**–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:**
- **Trend Strength (40% –≤–µ—Å–∞):** ADX/DMI, Moving Averages, Ichimoku
- **Momentum (30% –≤–µ—Å–∞):** RSI, Stochastic, MACD  
- **Price Action (30% –≤–µ—Å–∞):** HH/LL, Heikin-Ashi, Candle Range
- **Market Activity (10% –≤–µ—Å–∞):** Bollinger Bands, ATR, Volume

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å 4 –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏ –∞–Ω–∞–ª–∏–∑–∞
- –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
- –ì–æ—Ç–æ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è BTC
- –í—ã—Å–æ–∫–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

### 2. trend_classifier_iziceros - –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
**–û–ø–∏—Å–∞–Ω–∏–µ:** Python –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤

**–ê–ª–≥–æ—Ä–∏—Ç–º:**
- –°–∫–æ–ª—å–∑—è—â–∏–µ –æ–∫–Ω–∞ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
- –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–∫–Ω–∞
- –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–∞ –ø–æ –Ω–∞–∫–ª–æ–Ω—É –∏ —Å–º–µ—â–µ–Ω–∏—é

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `N` - —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 24)
- `alpha` - –ø–æ—Ä–æ–≥ –∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω–∞–∫–ª–æ–Ω–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2.0)
- `beta` - –ø–æ—Ä–æ–≥ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å–º–µ—â–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2.0)
- `overlap_ratio` - –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.33)

**–ü—Ä–æ–±–ª–µ–º—ã:**
- –ü–µ—Ä–µ–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è (R¬≤ = -5.7%)
- –°–ª–æ–∂–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ (4+ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞)
- –ò–∑–±—ã—Ç–æ—á–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è

### 3. trading_classifier.pine - –ü–æ–ª–æ—Å—ã —Ç—Ä–µ–Ω–¥–∞
**–û–ø–∏—Å–∞–Ω–∏–µ:** Pine Script –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä —Å —Å–∏—Å—Ç–µ–º–æ–π –ø–æ–ª–æ—Å —Ç—Ä–µ–Ω–¥–∞

**–ê–ª–≥–æ—Ä–∏—Ç–º:**
- SMEMA (Smooth EMA) - –∫–æ–º–±–∏–Ω–∞—Ü–∏—è SMA –∏ EMA
- 6 —É—Ä–æ–≤–Ω–µ–π –ø–æ–ª–æ—Å (3 –≤–≤–µ—Ä—Ö, 3 –≤–Ω–∏–∑)
- –°–∏–ª–∞ —Ç—Ä–µ–Ω–¥–∞ –ø–æ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è–º

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `length` - –ø–µ—Ä–∏–æ–¥ –¥–ª—è SMEMA (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 10)

**–†–µ–∑—É–ª—å—Ç–∞—Ç—ã:**
- 79% —Ä–∞–∑–Ω–æ—Å—Ç—å –≤ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
- –ü—Ä–æ—Å—Ç–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ (1 –ø–∞—Ä–∞–º–µ—Ç—Ä)
- –°—Ç–∞–±–∏–ª—å–Ω–∞—è —Ä–∞–±–æ—Ç–∞

## üî¨ –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –æ—Ç–±–æ—Ä–∞

### –≠—Ç–∞–ø 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è

#### 1.1 –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
```python
# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö BTC –¥–ª—è –≤—Å–µ—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
timeframes = ['15m', '30m', '1h', '4h', '1d']
data = {}

for tf in timeframes:
    file_path = f'../../indicators/data_frames/df_btc_{tf}.csv'
    df = pd.read_csv(file_path)
    df['timestamps'] = pd.to_datetime(df['timestamps'])
    df.set_index('timestamps', inplace=True)
    data[tf] = df
```

#### 1.2 –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
```python
def analyze_data_quality(data):
    """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
    quality_report = {}
    
    for tf, df in data.items():
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤
        missing_values = df.isnull().sum().sum()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        duplicates = df.index.duplicated().sum()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤
        price_mean = df['close'].mean()
        price_std = df['close'].std()
        outliers = df[abs(df['close'] - price_mean) > 3 * price_std]
        
        quality_report[tf] = {
            'records': len(df),
            'missing_values': missing_values,
            'duplicates': duplicates,
            'outliers': len(outliers)
        }
    
    return quality_report
```

#### 1.3 –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
```python
def validate_data(data):
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    required_columns = ['open', 'high', 'low', 'close', 'volume']
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
    if not all(col in data.columns for col in required_columns):
        missing_cols = [col for col in required_columns if col not in data.columns]
        raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing_cols}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–æ–≥–∏—á–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
    if not (data['high'] >= data['low']).all():
        raise ValueError("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: high < low")
    
    return True
```

### –≠—Ç–∞–ø 2: –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤

#### 2.1 –ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤
```python
class BaseMarketZoneClassifier(ABC):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ —Ä—ã–Ω–æ—á–Ω—ã—Ö –∑–æ–Ω"""
    
    def __init__(self, name: str, parameters: Dict = None):
        self.name = name
        self.parameters = parameters or {}
        self.is_fitted = False
        self.classes_ = None
    
    @abstractmethod
    def fit(self, data: pd.DataFrame) -> 'BaseMarketZoneClassifier':
        """–û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö"""
        pass
    
    @abstractmethod
    def predict(self, data: pd.DataFrame) -> np.ndarray:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä—ã–Ω–æ—á–Ω—ã—Ö –∑–æ–Ω"""
        pass
    
    @abstractmethod
    def get_feature_importance(self) -> Dict[str, float]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        pass
```

#### 2.2 Market Zone Analyzer (MZA)
```python
class MZAClassifier(BaseMarketZoneClassifier):
    """Market Zone Analyzer - —á–µ—Ç—ã—Ä–µ—Ö—É—Ä–æ–≤–Ω–µ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞"""
    
    def __init__(self, parameters: Dict = None):
        default_params = {
            # Trend Indicators
            'adx_length': 21,
            'adx_threshold': 18,
            'fast_ma_length': 21,
            'slow_ma_length': 55,
            
            # Momentum Indicators
            'rsi_length': 21,
            'stoch_length': 21,
            'macd_fast': 12,
            'macd_slow': 26,
            'macd_signal': 9,
            
            # Price Action Indicators
            'hh_ll_range': 21,
            'ha_doji_range': 8,
            'candle_range_length': 13,
            
            # Market Activity Indicators
            'bb_length': 20,
            'bb_multiplier': 2.2,
            'atr_length': 14,
            'volume_ma_length': 20,
            
            # Weights (–∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞)
            'trend_weight': 0.4,
            'momentum_weight': 0.3,
            'price_action_weight': 0.3
        }
        
        super().__init__("Market Zone Analyzer", default_params)
    
    def _calculate_indicators(self, data: pd.DataFrame) -> None:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤—Å–µ—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        # Trend Indicators
        self._calculate_adx_dmi(data)
        self._calculate_moving_averages(data)
        
        # Momentum Indicators
        self._calculate_rsi(data)
        self._calculate_stochastic(data)
        self._calculate_macd(data)
        
        # Price Action Indicators
        self._calculate_hh_ll(data)
        self._calculate_heikin_ashi(data)
        self._calculate_candle_range(data)
        
        # Market Activity Indicators
        self._calculate_bollinger_bands(data)
        self._calculate_atr(data)
        self._calculate_volume_indicators(data)
    
    def _calculate_adaptive_weights(self, data: pd.DataFrame) -> None:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö –≤–µ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏"""
        volatility = self.calculate_volatility(data, window=20)
        
        high_vol_threshold = self.parameters['high_volatility_threshold']
        low_vol_threshold = self.parameters['low_volatility_threshold']
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        high_vol_mask = volatility > high_vol_threshold
        low_vol_mask = volatility < low_vol_threshold
        
        self.parameters['trend_weight'] = np.where(high_vol_mask, 0.5, 
                                                   np.where(low_vol_mask, 0.25, 0.4))
        self.parameters['momentum_weight'] = np.where(high_vol_mask, 0.35,
                                                       np.where(low_vol_mask, 0.20, 0.30))
        self.parameters['price_action_weight'] = np.where(high_vol_mask, 0.15,
                                                          np.where(low_vol_mask, 0.55, 0.30))
```

#### 2.3 trend_classifier_iziceros
```python
class TrendClassifier(BaseMarketZoneClassifier):
    """–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤"""
    
    def __init__(self, parameters: Dict = None):
        default_params = {
            'N': 24,           # —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞
            'alpha': 2.0,      # –ø–æ—Ä–æ–≥ –∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω–∞–∫–ª–æ–Ω–∞
            'beta': 2.0,       # –ø–æ—Ä–æ–≥ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å–º–µ—â–µ–Ω–∏—è
            'overlap_ratio': 0.33  # –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
        }
        
        super().__init__("Trend Classifier", default_params)
    
    def _calculate_segments(self, data: pd.DataFrame) -> None:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞"""
        n = self.parameters['N']
        overlap_ratio = self.parameters['overlap_ratio']
        alpha = self.parameters['alpha']
        beta = self.parameters['beta']
        
        offset = max(1, int(n * overlap_ratio))
        
        segments = []
        current_segment = {
            'start': 0,
            'slopes': [],
            'offsets': [],
            'starts': []
        }
        
        prev_fit = None
        
        for start in range(0, len(data) - n, offset):
            end = start + n
            
            x_window = np.array(range(start, end))
            y_window = np.array(data['close'][start:end])
            fit = np.polyfit(x_window, y_window, 1)
            
            current_segment['slopes'].append(fit[0])
            current_segment['offsets'].append(fit[1])
            current_segment['starts'].append(start)
            
            if prev_fit is not None:
                slope_change = abs(fit[0] - prev_fit[0])
                offset_change = abs(fit[1] - prev_fit[1])
                
                if slope_change > alpha or offset_change > beta:
                    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Å–µ–≥–º–µ–Ω—Ç
                    segment = {
                        'start': current_segment['start'],
                        'stop': start + offset // 2,
                        'slope': np.mean(current_segment['slopes']),
                        'offset': np.mean(current_segment['offsets'])
                    }
                    segments.append(segment)
                    
                    current_segment = {
                        'start': start + offset // 2,
                        'slopes': [],
                        'offsets': [],
                        'starts': []
                    }
            
            prev_fit = fit
        
        self.segments = segments
```

#### 2.4 trading_classifier.pine
```python
class TradingClassifier(BaseMarketZoneClassifier):
    """–ü–æ–ª–æ—Å—ã —Ç—Ä–µ–Ω–¥–∞"""
    
    def __init__(self, parameters: Dict = None):
        default_params = {
            'length': 10  # –ø–µ—Ä–∏–æ–¥ –¥–ª—è SMEMA
        }
        
        super().__init__("Trading Classifier", default_params)
    
    def _calculate_smema(self, data: pd.DataFrame) -> pd.Series:
        """Smooth EMA - –∫–æ–º–±–∏–Ω–∞—Ü–∏—è SMA –∏ EMA"""
        length = self.parameters['length']
        ema_val = data['close'].ewm(span=length).mean()
        smema_val = ema_val.rolling(window=length).mean()
        return smema_val
    
    def _calculate_trend_bands(self, data: pd.DataFrame) -> Dict[str, pd.Series]:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ–ª–æ—Å —Ç—Ä–µ–Ω–¥–∞"""
        smema_line = self._calculate_smema(data)
        
        # –í—ã—á–∏—Å–ª—è–µ–º step
        step = self._calculate_smema(data[['high', 'low']].diff().abs())
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–æ—Å—ã
        bands = {
            'up3': smema_line + step * 3,
            'up2': smema_line + step * 2,
            'up1': smema_line + step,
            'dn1': smema_line - step,
            'dn2': smema_line - step * 2,
            'dn3': smema_line - step * 3
        }
        
        return bands
    
    def _calculate_trend_strength(self, data: pd.DataFrame, bands: Dict[str, pd.Series]) -> np.ndarray:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–∏–ª—ã —Ç—Ä–µ–Ω–¥–∞"""
        close = data['close']
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è —Å –ø–æ–ª–æ—Å–∞–º–∏
        above3 = close > bands['up3']
        above2 = close > bands['up2']
        above1 = close > bands['up1']
        
        below1 = close < bands['dn1']
        below2 = close < bands['dn2']
        below3 = close < bands['dn3']
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å–∏–ª—É –±—ã—á—å–µ–≥–æ –∏ –º–µ–¥–≤–µ–∂—å–µ–≥–æ —Ç—Ä–µ–Ω–¥–∞
        bull_strength = (above1.astype(int) + above2.astype(int) + above3.astype(int))
        bear_strength = (below1.astype(int) + below2.astype(int) + below3.astype(int))
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞
        trend = smema_line > smema_line.shift(1)
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        predictions = np.where(
            trend & (bull_strength >= 1), 1,      # –ë—ã—á–∏–π
            np.where(~trend & (bear_strength >= 1), -1, 0)  # –ú–µ–¥–≤–µ–∂–∏–π, –ë–æ–∫–æ–≤–æ–π
        )
        
        return predictions
```

### –≠—Ç–∞–ø 3: –ú–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞

#### 3.1 –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
```python
def calculate_classification_metrics(y_true, y_pred):
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
    
    metrics = {
        'accuracy': accuracy_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred, average='weighted'),
        'recall': recall_score(y_true, y_pred, average='weighted'),
        'f1_score': f1_score(y_true, y_pred, average='weighted')
    }
    
    return metrics
```

#### 3.2 –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
```python
def calculate_time_series_metrics(data, predictions):
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤"""
    # –í—ã—á–∏—Å–ª—è–µ–º –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å
    returns = data['close'].pct_change().dropna()
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º
    bull_mask = predictions == 1
    bear_mask = predictions == -1
    sideways_mask = predictions == 0
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω—é—é –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –≤ –∫–∞–∂–¥–æ–º —Ä–µ–∂–∏–º–µ
    bull_return = returns[bull_mask].mean() if bull_mask.any() else 0
    bear_return = returns[bear_mask].mean() if bear_mask.any() else 0
    sideways_return = returns[sideways_mask].mean() if sideways_mask.any() else 0
    
    # –í—ã—á–∏—Å–ª—è–µ–º –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
    bull_vol = returns[bull_mask].std() if bull_mask.any() else 0
    bear_vol = returns[bear_mask].std() if bear_mask.any() else 0
    sideways_vol = returns[sideways_mask].std() if sideways_mask.any() else 0
    
    metrics = {
        'bull_return': bull_return,
        'bear_return': bear_return,
        'sideways_return': sideways_return,
        'bull_volatility': bull_vol,
        'bear_volatility': bear_vol,
        'sideways_volatility': sideways_vol,
        'return_spread': bull_return - bear_return
    }
    
    return metrics
```

#### 3.3 –ê–Ω–∞–ª–∏–∑ look-ahead bias
```python
def analyze_look_ahead_bias(data, classifier, n_splits=5):
    """–ê–Ω–∞–ª–∏–∑ look-ahead bias —Å –ø–æ–º–æ—â—å—é walk-forward –∞–Ω–∞–ª–∏–∑–∞"""
    from sklearn.model_selection import TimeSeriesSplit
    
    tscv = TimeSeriesSplit(n_splits=n_splits)
    
    results = []
    
    for train_idx, test_idx in tscv.split(data):
        train_data = data.iloc[train_idx]
        test_data = data.iloc[test_idx]
        
        # –û–±—É—á–∞–µ–º –Ω–∞ train_data
        classifier.fit(train_data)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞ test_data
        predictions = classifier.predict(test_data)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        metrics = calculate_time_series_metrics(test_data, predictions)
        results.append(metrics)
    
    return results
```

### –≠—Ç–∞–ø 4: –ú—É–ª—å—Ç–∏—Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑

#### 4.1 –ê–Ω–∞–ª–∏–∑ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å–∏–≥–Ω–∞–ª–æ–≤
```python
def analyze_signal_consistency(data_dict, classifier):
    """–ê–Ω–∞–ª–∏–∑ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å–∏–≥–Ω–∞–ª–æ–≤ –º–µ–∂–¥—É —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º–∏"""
    predictions_dict = {}
    
    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
    for tf, df in data_dict.items():
        classifier.fit(df)
        predictions = classifier.predict(df)
        predictions_dict[tf] = predictions
    
    # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º–∏
    correlation_matrix = pd.DataFrame(index=data_dict.keys(), 
                                   columns=data_dict.keys())
    
    for tf1 in data_dict.keys():
        for tf2 in data_dict.keys():
            if tf1 != tf2:
                # –ù–∞—Ö–æ–¥–∏–º –æ–±—â–∏–µ –ø–µ—Ä–∏–æ–¥—ã
                common_periods = data_dict[tf1].index.intersection(data_dict[tf2].index)
                
                if len(common_periods) > 0:
                    pred1 = predictions_dict[tf1][data_dict[tf1].index.isin(common_periods)]
                    pred2 = predictions_dict[tf2][data_dict[tf2].index.isin(common_periods)]
                    
                    correlation = np.corrcoef(pred1, pred2)[0, 1]
                    correlation_matrix.loc[tf1, tf2] = correlation
    
    return correlation_matrix
```

#### 4.2 Lead/Lag –∞–Ω–∞–ª–∏–∑
```python
def analyze_lead_lag(data_dict, classifier):
    """–ê–Ω–∞–ª–∏–∑ –æ–ø–µ—Ä–µ–∂–∞—é—â–∏—Ö –∏ –∑–∞–ø–∞–∑–¥—ã–≤–∞—é—â–∏—Ö —Å–∏–≥–Ω–∞–ª–æ–≤"""
    lead_lag_results = {}
    
    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ç–∞–π–º—Ñ—Ä–µ–π–º —Å –¥–Ω–µ–≤–Ω—ã–º
    daily_data = data_dict['1d']
    daily_classifier = classifier.fit(daily_data)
    daily_predictions = daily_classifier.predict(daily_data)
    
    for tf, df in data_dict.items():
        if tf != '1d':
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
            tf_classifier = classifier.fit(df)
            tf_predictions = tf_classifier.predict(df)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –ª–∞–≥–∞–º–∏
            lags = range(-5, 6)  # –æ—Ç -5 –¥–æ +5 –ø–µ—Ä–∏–æ–¥–æ–≤
            correlations = []
            
            for lag in lags:
                if lag > 0:
                    # –¢–µ–∫—É—â–∏–π —Ç–∞–π–º—Ñ—Ä–µ–π–º –æ–ø–µ—Ä–µ–∂–∞–µ—Ç –¥–Ω–µ–≤–Ω–æ–π
                    corr = np.corrcoef(tf_predictions[:-lag], daily_predictions[lag:])[0, 1]
                elif lag < 0:
                    # –î–Ω–µ–≤–Ω–æ–π –æ–ø–µ—Ä–µ–∂–∞–µ—Ç —Ç–µ–∫—É—â–∏–π —Ç–∞–π–º—Ñ—Ä–µ–π–º
                    corr = np.corrcoef(tf_predictions[-lag:], daily_predictions[:lag])[0, 1]
                else:
                    # –ë–µ–∑ –ª–∞–≥–∞
                    corr = np.corrcoef(tf_predictions, daily_predictions)[0, 1]
                
                correlations.append(corr)
            
            lead_lag_results[tf] = {
                'lags': lags,
                'correlations': correlations,
                'best_lag': lags[np.argmax(correlations)],
                'max_correlation': max(correlations)
            }
    
    return lead_lag_results
```

### –≠—Ç–∞–ø 5: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

#### 5.1 Grid Search
```python
def grid_search_optimization(data, classifier_class, param_grid):
    """Grid Search –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
    from sklearn.model_selection import ParameterGrid
    
    best_score = -np.inf
    best_params = None
    best_classifier = None
    
    for params in ParameterGrid(param_grid):
        classifier = classifier_class(parameters=params)
        
        # –û–±—É—á–∞–µ–º –∏ —Ç–µ—Å—Ç–∏—Ä—É–µ–º
        classifier.fit(data)
        predictions = classifier.predict(data)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å–∫–æ—Ä
        score = calculate_classification_score(data, predictions)
        
        if score > best_score:
            best_score = score
            best_params = params
            best_classifier = classifier
    
    return best_classifier, best_params, best_score
```

#### 5.2 Bayesian Optimization
```python
def bayesian_optimization(data, classifier_class, param_bounds, n_iterations=50):
    """Bayesian Optimization –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
    from skopt import gp_minimize
    from skopt.space import Real, Integer
    
    def objective(params):
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ —Å–ª–æ–≤–∞—Ä—å
        param_dict = dict(zip(param_bounds.keys(), params))
        
        classifier = classifier_class(parameters=param_dict)
        classifier.fit(data)
        predictions = classifier.predict(data)
        
        score = calculate_classification_score(data, predictions)
        return -score  # –ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π —Å–∫–æ—Ä
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    space = []
    for param_name, (low, high) in param_bounds.items():
        if isinstance(low, int):
            space.append(Integer(low, high))
        else:
            space.append(Real(low, high))
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é
    result = gp_minimize(objective, space, n_calls=n_iterations)
    
    best_params = dict(zip(param_bounds.keys(), result.x))
    best_score = -result.fun
    
    return best_params, best_score
```

### –≠—Ç–∞–ø 6: –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑

#### 6.1 –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã
```python
def statistical_comparison(results_dict):
    """–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    from scipy import stats
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    metrics = ['accuracy', 'f1_score', 'return_spread']
    
    comparison_results = {}
    
    for metric in metrics:
        values = [results[metric] for results in results_dict.values()]
        classifiers = list(results_dict.keys())
        
        # t-test –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å—Ä–µ–¥–Ω–∏—Ö
        t_stat, p_value = stats.ttest_ind(values[0], values[1])
        
        # Mann-Whitney U test –¥–ª—è –Ω–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        u_stat, u_p_value = stats.mannwhitneyu(values[0], values[1])
        
        comparison_results[metric] = {
            't_statistic': t_stat,
            't_p_value': p_value,
            'u_statistic': u_stat,
            'u_p_value': u_p_value,
            'significant': p_value < 0.05
        }
    
    return comparison_results
```

#### 6.2 –ö—Ä–∏—Ç–µ—Ä–∏–∏ –≤—ã–±–æ—Ä–∞ –ª—É—á—à–µ–≥–æ —Ä–µ—à–µ–Ω–∏—è
```python
def select_best_classifier(results_dict):
    """–í—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤"""
    
    # –í–µ—Å–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
    weights = {
        'accuracy': 0.25,
        'f1_score': 0.25,
        'return_spread': 0.20,
        'stability': 0.15,
        'speed': 0.10,
        'interpretability': 0.05
    }
    
    scores = {}
    
    for classifier_name, results in results_dict.items():
        score = 0
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ (0-1)
        normalized_accuracy = results['accuracy']
        normalized_f1 = results['f1_score']
        normalized_return = min(results['return_spread'] / 0.1, 1)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ 10%
        
        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–∏–π —Å–∫–æ—Ä
        score = (
            normalized_accuracy * weights['accuracy'] +
            normalized_f1 * weights['f1_score'] +
            normalized_return * weights['return_spread'] +
            results['stability'] * weights['stability'] +
            results['speed'] * weights['speed'] +
            results['interpretability'] * weights['interpretability']
        )
        
        scores[classifier_name] = score
    
    # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π
    best_classifier = max(scores, key=scores.get)
    best_score = scores[best_classifier]
    
    return best_classifier, best_score, scores
```

## üìä –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

### 1. –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
- **–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞** –≤—Å–µ—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤
- **–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã** –¥–ª—è –∫–∞–∂–¥–æ–≥–æ
- **–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞** —Å —É—á–µ—Ç–æ–º look-ahead bias
- **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏** –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é

### 2. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
- **–ì–æ—Ç–æ–≤—ã–π –∫–æ–¥** –¥–ª—è –≤—Å–µ—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤
- **Jupyter notebooks** —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
- **–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è** –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é
- **TradingView –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è** –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–µ—à–µ–Ω–∏—è

### 3. –ù–∞—É—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
- **–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑** —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
- **–ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è** –æ—Ü–µ–Ω–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ —Ä—ã–Ω–∫–∞
- **Best practices** –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø–µ—Ä–µ–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
- **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏** –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π

## ‚ö†Ô∏è –í–∞–∂–Ω—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è

### –ò–∑–±–µ–∂–∞–Ω–∏–µ look-ahead bias
- **–°—Ç—Ä–æ–≥–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ** train/validation/test
- **–í—Ä–µ–º–µ–Ω–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å** –¥–∞–Ω–Ω—ã—Ö
- **Out-of-sample —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ**
- **Walk-forward –∞–Ω–∞–ª–∏–∑**

### –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å
- **–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã** –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞
- **Bootstrap confidence intervals**
- **Effect size** –∞–Ω–∞–ª–∏–∑
- **Practical significance** –æ—Ü–µ–Ω–∫–∞

### –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å
- **–§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ random seeds**
- **–î–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ** –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- **Version control** –¥–ª—è –∫–æ–¥–∞
- **–î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ** —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤

---

**–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** 24.10.2025  
**–ê–≤—Ç–æ—Ä:** AI Assistant  
**–°—Ç–∞—Ç—É—Å:** –í —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ  
**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** –í—ã—Å–æ–∫–∏–π
