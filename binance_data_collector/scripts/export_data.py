"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
"""

import sys
from pathlib import Path
import pandas as pd

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
sys.path.append(str(Path(__file__).parent.parent.parent))

from binance_data_collector.config import config
from binance_data_collector.utils.logger import setup_logger
from binance_data_collector.utils.file_handler import load_data, save_data

logger = setup_logger("binance_export")

def convert_timeframe_data(
    input_file: Path,
    output_format: str = "csv",
    output_dir: Path = None
) -> None:
    """
    –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –¥—Ä—É–≥–æ–π —Ñ–æ—Ä–º–∞—Ç
    
    Args:
        input_file: –ü—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É
        output_format: –¶–µ–ª–µ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç (csv, json, parquet, xlsx)
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é processed/)
    """
    if output_dir is None:
        output_dir = config.DATA_DIR / "processed"
    
    logger.info(f"üì§ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è {input_file.name} -> {output_format}")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    source_format = input_file.suffix[1:].lower()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    df = load_data(input_file, format=source_format)
    
    if df.empty:
        logger.warning(f"‚ö†Ô∏è –§–∞–π–ª –ø—É—Å—Ç: {input_file}")
        return
    
    # –°–æ–∑–¥–∞–µ–º –∏–º—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    output_file = output_dir / f"{input_file.stem}.{output_format}"
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –Ω–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
    save_data(df, output_file, format=output_format)
    
    logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {output_file}")

def batch_convert(
    input_dir: Path,
    output_format: str = "parquet",
    output_dir: Path = None
) -> None:
    """
    –ü–∞–∫–µ—Ç–Ω–∞—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    
    Args:
        input_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏
        output_format: –¶–µ–ª–µ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    """
    if output_dir is None:
        output_dir = config.DATA_DIR / "processed"
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"üì¶ –ü–∞–∫–µ—Ç–Ω–∞—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∏–∑ {input_dir} –≤ {output_format}")
    
    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ CSV —Ñ–∞–π–ª—ã
    csv_files = list(input_dir.glob("*.csv"))
    
    if not csv_files:
        logger.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ CSV —Ñ–∞–π–ª–æ–≤ –≤ {input_dir}")
        return
    
    logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(csv_files)} —Ñ–∞–π–ª–æ–≤")
    
    for csv_file in csv_files:
        try:
            convert_timeframe_data(csv_file, output_format, output_dir)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ {csv_file.name}: {e}")
    
    logger.info("‚úÖ –ü–∞–∫–µ—Ç–Ω–∞—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse
    
    parser = argparse.ArgumentParser(description="–≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö Binance –≤ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã")
    parser.add_argument(
        "--input-dir",
        type=str,
        default=str(config.DATA_DIR / "historical"),
        help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏"
    )
    parser.add_argument(
        "--output-format",
        type=str,
        choices=["csv", "json", "parquet", "xlsx"],
        default="parquet",
        help="–¶–µ–ª–µ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default=str(config.DATA_DIR / "processed"),
        help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è"
    )
    
    args = parser.parse_args()
    
    batch_convert(
        Path(args.input_dir),
        args.output_format,
        Path(args.output_dir)
    )

if __name__ == "__main__":
    main()


